"""
åŸºäº UAV å…‰è°±ç‰¹å¾çš„åœŸå£¤ç›åˆ†åæ¼”ï¼ˆçš®å°”é€Šç›¸å…³æ€§ + RFEç‰¹å¾é€‰æ‹© + å¤šæ¨¡å‹ï¼‰

æµç¨‹åŒ…å«ï¼š
1. æ•°æ®è¯»å–ã€ç‰¹å¾é€‰æ‹©ï¼ˆç¬¬ 9 åˆ—èµ·çš„ 30 ä¸ª UAV æŒ‡æ•°ï¼‰
2. çš®å°”é€Šç›¸å…³æ€§å’ŒRFEç‰¹å¾é€‰æ‹©ï¼ˆæ¯”è¾ƒä¸¤ç§æ–¹æ³•ï¼‰
3. å¤šä¸ªæ¨¡å‹è®­ç»ƒï¼ˆSVR, RandomForest, PLSR, XGBoost, GradientBoostingï¼‰
4. æ¨¡å‹è¯„ä¼°ä¸å¯è§†åŒ–
5. å¯¼å‡ºå¤š Sheet Excel + é¢„æµ‹ç»“æœ
æ ·ç‚¹æ•°æ®(138ä¸ª)
    â†“ è¿‡æ»¤ï¼ˆæ’é™¤å¯Œå¹³FPæ ·ç‚¹ï¼‰
    â†“ æå–30ä¸ªå…‰è°±æŒ‡æ•°ç‰¹å¾
    â†“
ç‰¹å¾é€‰æ‹©ï¼ˆåŒé‡ç­–ç•¥ï¼‰
    â”œâ”€ çš®å°”é€Šï¼š|r| >= 0.3
    â””â”€ RFEï¼šä¿ç•™15ä¸ª
    â†“ æ™ºèƒ½èåˆï¼ˆäº¤é›†/å¹¶é›†ï¼‰
    â†“
å¤šæ¨¡å‹è®­ç»ƒ
    â”œâ”€ SVRï¼ˆç½‘æ ¼æœç´¢è°ƒå‚ï¼‰
    â”œâ”€ RandomForestï¼ˆ200æ£µæ ‘ï¼‰
    â”œâ”€ PLSRï¼ˆè‡ªåŠ¨é€‰ä¸»æˆåˆ†ï¼‰
    â”œâ”€ GradientBoosting
    â””â”€ XGBoostï¼ˆå¯é€‰ï¼‰
    â†“
æ¨¡å‹è¯„ä¼°
    â”œâ”€ RÂ², RMSE, MAEç­‰11é¡¹æŒ‡æ ‡
    â”œâ”€ æ•£ç‚¹å›¾
    â””â”€ é€‰æœ€ä¼˜æ¨¡å‹
    â†“
ç©ºé—´é¢„æµ‹ï¼ˆå¯é€‰ï¼‰
    â”œâ”€ è¯»å–30æ³¢æ®µå½±åƒ
    â”œâ”€ é€åƒå…ƒé¢„æµ‹
    â”œâ”€ è¿‡æ»¤èƒŒæ™¯
    â””â”€ ç”Ÿæˆåˆ†å¸ƒå›¾
    â†“
ç»“æœè¾“å‡º
    â”œâ”€ Excelå¤šè¡¨æ ¼æŠ¥å‘Š
    â”œâ”€ å¯è§†åŒ–å›¾è¡¨
    â””â”€ é¢„æµ‹æ …æ ¼
"""

from __future__ import annotations
from datetime import datetime
from pathlib import Path

from matplotlib import rcParams
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import rasterio
from scipy.stats import pearsonr, spearmanr
import seaborn as sns
from sklearn.cross_decomposition import PLSRegression
from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor
from sklearn.feature_selection import RFE
from sklearn.metrics import (
    explained_variance_score,
    mean_absolute_error,
    mean_squared_error,
    median_absolute_error,
    r2_score,
)
from sklearn.model_selection import GridSearchCV, cross_val_score, train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.svm import SVR

try:
    from xgboost import XGBRegressor
    HAS_XGBOOST = True
except ImportError:
    HAS_XGBOOST = False
    print("âš ï¸ XGBoostæœªå®‰è£…ï¼Œå°†è·³è¿‡XGBoostæ¨¡å‹")

rcParams["font.family"] = ["SimHei"]
rcParams["axes.unicode_minus"] = False

# ================= ç”¨æˆ·è¾“å…¥åŒºåŸŸ =================
# input_csv = Path(
#   r"D:\å¯Œå¹³æ˜Ÿæœºå…‰è°±èåˆåæ¼”\middata\Sim_samples_extracted_values.csv_with_indices_v2.csv"
# )
input_csv = Path(
  r"D:\å¯Œå¹³æ˜Ÿæœºå…‰è°±èåˆåæ¼”\middata\S2_samples_extracted_values.csv_with_indices_.csv"
)
output_dir = Path(
    r"D:\å¯Œå¹³æ˜Ÿæœºå…‰è°±èåˆåæ¼”\ç»“æœv2\plsr\S2_plsr_vip_multimodel_å…¨ç›é‡_æ’é™¤å¯Œå¹³"
)
target_column = "å…¨ç›é‡"

feature_count = 30  # é€‰æ‹©æœ€å30åˆ—ä½œä¸ºç‰¹å¾
test_ratio = 0.2
random_state = 42

# æ ·æœ¬è¿‡æ»¤å‚æ•°ï¼ˆæ ¹æ®åˆ—ä¸­åŒ…å«çš„å­—ç¬¦ä¸²æ’é™¤æ ·æœ¬ï¼‰
filter_column = "æŠ½æ ·ç¼–"  # ç”¨äºè¿‡æ»¤çš„åˆ—åï¼Œå¦‚æœä¸º None åˆ™ä¸è¿›è¡Œè¿‡æ»¤
exclude_strings = ["FP"]  # è¦æ’é™¤çš„å­—ç¬¦ä¸²åˆ—è¡¨ï¼Œä¾‹å¦‚ï¼š["A1", "B2"] è¡¨ç¤ºæ’é™¤åˆ—å€¼ä¸­åŒ…å« "A1" æˆ– "B2" çš„æ ·æœ¬

# ç‰¹å¾é€‰æ‹©å‚æ•°
pearson_threshold = 0.3  # çš®å°”é€Šç›¸å…³ç³»æ•°é˜ˆå€¼ï¼ˆç»å¯¹å€¼ï¼‰
rfe_n_features = 15  # RFEé€‰æ‹©çš„ç‰¹å¾æ•°é‡
rfe_estimator = "RandomForest"  # RFEä½¿ç”¨çš„ä¼°è®¡å™¨ç±»å‹: "RandomForest", "SVR", "PLSR"
cv_folds = 5  # äº¤å‰éªŒè¯æŠ˜æ•°

# SVR GridSearchå‚æ•°
svr_param_grid = {
    "C": [0.1, 1, 10, 100],
    "gamma": ["scale", "auto", 0.001, 0.01, 0.1],
    "epsilon": [0.01, 0.1, 0.5],
}

excel_name = "salinity_results_plsr_vip.xlsx"

# ç©ºé—´åˆ†å¸ƒå›¾ç”Ÿæˆå‚æ•°
generate_spatial_map = False  # æ˜¯å¦ç”Ÿæˆç©ºé—´åˆ†å¸ƒå›¾
raster_image_path = Path(
    r"D:\å¯Œå¹³æ˜Ÿæœºå…‰è°±èåˆåæ¼”\æ•°æ®\S2_features_30bands.tif"
)  # ç”¨äºé¢„æµ‹çš„æ …æ ¼å½±åƒè·¯å¾„
spatial_map_model = "GradientBoosting"  # ç”¨äºç©ºé—´é¢„æµ‹çš„æ¨¡å‹åç§°ï¼ˆä»è®­ç»ƒå¥½çš„æ¨¡å‹ä¸­é€‰æ‹©ï¼Œå¦‚ "SVR", "RandomForest", "PLSR" ç­‰ï¼‰
output_salinity_raster = output_dir / "S2salinity_predictionGradientBoosting.tif"  # è¾“å‡ºçš„ç›åˆ†é¢„æµ‹æ …æ ¼è·¯å¾„

# èƒŒæ™¯åƒå…ƒå»é™¤å‚æ•°
remove_background_pixels = True  # æ˜¯å¦å»é™¤èƒŒæ™¯åƒå…ƒ
background_threshold = None  # èƒŒæ™¯é˜ˆå€¼ï¼ˆå¦‚æœæ‰€æœ‰æ³¢æ®µå€¼éƒ½å°äºæ­¤å€¼ï¼Œåˆ™è§†ä¸ºèƒŒæ™¯ï¼‰ï¼ŒNoneè¡¨ç¤ºä¸å¯ç”¨
check_nodata_values = True  # æ˜¯å¦æ£€æŸ¥åŸå§‹æ …æ ¼çš„nodataå€¼
# =================================================


def ensure_paths() -> None:
    if not input_csv.exists():
        raise FileNotFoundError(f"æœªæ‰¾åˆ°è¾“å…¥ CSVï¼š{input_csv}")
    output_dir.mkdir(parents=True, exist_ok=True)


def select_features(df: pd.DataFrame) -> pd.DataFrame:
    """é€‰æ‹©æœ€å30åˆ—ä½œä¸ºç‰¹å¾"""
    if len(df.columns) < feature_count:
        raise ValueError(
            f"åˆ—æ•°ä¸è¶³ï¼šéœ€è¦è‡³å°‘ {feature_count} åˆ—ï¼Œå½“å‰ {len(df.columns)} åˆ—ã€‚"
        )
    features = df.iloc[:, -feature_count:]
    if features.shape[1] != feature_count:
        raise ValueError(f"ç‰¹å¾æ•°é‡ä¸è¶³ {feature_count} åˆ—ï¼Œè¯·æ£€æŸ¥è¾“å…¥æ•°æ®ã€‚")
    return features


def select_features_by_pearson(
    X: np.ndarray, y: np.ndarray, feature_names: list, threshold: float = 0.3
) -> tuple[list, np.ndarray]:
    """
    åŸºäºçš®å°”é€Šç›¸å…³ç³»æ•°é€‰æ‹©ç‰¹å¾
    
    å‚æ•°:
        X: ç‰¹å¾çŸ©é˜µ
        y: ç›®æ ‡å˜é‡
        feature_names: ç‰¹å¾åç§°åˆ—è¡¨
        threshold: ç›¸å…³ç³»æ•°é˜ˆå€¼ï¼ˆç»å¯¹å€¼ï¼‰
    
    è¿”å›:
        selected_features: é€‰ä¸­çš„ç‰¹å¾åç§°åˆ—è¡¨
        selected_indices: é€‰ä¸­çš„ç‰¹å¾ç´¢å¼•
        correlation_scores: æ‰€æœ‰ç‰¹å¾çš„ç›¸å…³ç³»æ•°
    """
    print(f"\nğŸ“Š åŸºäºçš®å°”é€Šç›¸å…³ç³»æ•°é€‰æ‹©ç‰¹å¾ï¼ˆé˜ˆå€¼={threshold}ï¼‰...")
    
    correlations = np.zeros(X.shape[1])
    for i in range(X.shape[1]):
        corr, _ = pearsonr(X[:, i], y)
        correlations[i] = corr
    
    # é€‰æ‹©ç»å¯¹å€¼å¤§äºé˜ˆå€¼çš„ç‰¹å¾
    abs_correlations = np.abs(correlations)
    selected_mask = abs_correlations >= threshold
    selected_indices = np.where(selected_mask)[0]
    
    # å¦‚æœé€‰ä¸­çš„ç‰¹å¾å¤ªå°‘ï¼Œåˆ™é€‰æ‹©å‰top_nä¸ª
    if len(selected_indices) < 5:
        print(f"  âš ï¸ é˜ˆå€¼ç­›é€‰åç‰¹å¾æ•°è¿‡å°‘ï¼ˆ{len(selected_indices)}ä¸ªï¼‰ï¼Œæ”¹ä¸ºé€‰æ‹©å‰15ä¸ª")
        top_n = min(15, X.shape[1])
        selected_indices = np.argsort(abs_correlations)[::-1][:top_n]
        selection_method = f"å‰{top_n}ä¸ª"
    else:
        selection_method = f"|r|>={threshold}"
    
    selected_features = [feature_names[i] for i in selected_indices]
    
    print(f"  âœ… çš®å°”é€Šç›¸å…³æ€§é€‰æ‹©ï¼š{len(selected_features)} ä¸ªç‰¹å¾ï¼ˆ{selection_method}ï¼‰")
    print(f"  ç›¸å…³ç³»æ•°èŒƒå›´ï¼š[{correlations[selected_indices].min():.3f}, {correlations[selected_indices].max():.3f}]")
    
    return selected_features, selected_indices, correlations


def select_features_by_rfe(
    X: np.ndarray,
    y: np.ndarray,
    feature_names: list,
    n_features: int = 15,
    estimator_type: str = "RandomForest",
) -> tuple[list, np.ndarray]:
    """
    åŸºäºé€’å½’ç‰¹å¾æ¶ˆé™¤ï¼ˆRFEï¼‰é€‰æ‹©ç‰¹å¾
    
    å‚æ•°:
        X: ç‰¹å¾çŸ©é˜µ
        y: ç›®æ ‡å˜é‡
        feature_names: ç‰¹å¾åç§°åˆ—è¡¨
        n_features: è¦é€‰æ‹©çš„ç‰¹å¾æ•°é‡
        estimator_type: ä¼°è®¡å™¨ç±»å‹ ("RandomForest", "SVR", "PLSR")
    
    è¿”å›:
        selected_features: é€‰ä¸­çš„ç‰¹å¾åç§°åˆ—è¡¨
        selected_indices: é€‰ä¸­çš„ç‰¹å¾ç´¢å¼•
        rfe_ranking: RFEç‰¹å¾æ’åï¼ˆ1è¡¨ç¤ºæœ€é‡è¦ï¼‰
    """
    print(f"\nğŸ“Š åŸºäºRFEé€‰æ‹©ç‰¹å¾ï¼ˆç›®æ ‡ç‰¹å¾æ•°={n_features}ï¼Œä¼°è®¡å™¨={estimator_type}ï¼‰...")
    
    # é€‰æ‹©ä¼°è®¡å™¨
    if estimator_type == "RandomForest":
        estimator = RandomForestRegressor(
            n_estimators=100, random_state=random_state, n_jobs=-1
        )
    elif estimator_type == "SVR":
        scaler = StandardScaler()
        X_scaled = scaler.fit_transform(X)
        estimator = SVR(kernel="rbf", C=10, gamma="scale", epsilon=0.1)
        X = X_scaled
    elif estimator_type == "PLSR":
        scaler = StandardScaler()
        X_scaled = scaler.fit_transform(X)
        estimator = PLSRegression(n_components=min(10, X.shape[1], X.shape[0]))
        X = X_scaled
    else:
        raise ValueError(f"ä¸æ”¯æŒçš„ä¼°è®¡å™¨ç±»å‹: {estimator_type}")
    
    # é™åˆ¶ç‰¹å¾æ•°é‡ä¸è¶…è¿‡æ ·æœ¬æ•°å’Œç‰¹å¾æ•°
    n_features = min(n_features, X.shape[0], X.shape[1])
    
    # æ‰§è¡ŒRFE
    rfe = RFE(estimator=estimator, n_features_to_select=n_features, step=1)
    rfe.fit(X, y)
    
    # è·å–é€‰ä¸­çš„ç‰¹å¾
    selected_mask = rfe.support_
    selected_indices = np.where(selected_mask)[0]
    selected_features = [feature_names[i] for i in selected_indices]
    
    # è·å–ç‰¹å¾æ’åï¼ˆ1è¡¨ç¤ºæœ€é‡è¦ï¼‰
    rfe_ranking = rfe.ranking_
    
    print(f"  âœ… RFEé€‰æ‹©ï¼š{len(selected_features)} ä¸ªç‰¹å¾")
    print(f"  ç‰¹å¾æ’åèŒƒå›´ï¼š[{rfe_ranking[selected_indices].min()}, {rfe_ranking[selected_indices].max()}]")
    
    return selected_features, selected_indices, rfe_ranking


def compare_and_select_features(
    X: np.ndarray,
    y: np.ndarray,
    feature_names: list,
    pearson_threshold: float = 0.3,
    rfe_n_features: int = 15,
    rfe_estimator: str = "RandomForest",
) -> tuple[list, dict]:
    """
    æ¯”è¾ƒçš®å°”é€Šç›¸å…³æ€§å’ŒRFEä¸¤ç§æ–¹æ³•ï¼Œé€‰æ‹©æœ€ç»ˆç‰¹å¾
    
    å‚æ•°:
        X: ç‰¹å¾çŸ©é˜µ
        y: ç›®æ ‡å˜é‡
        feature_names: ç‰¹å¾åç§°åˆ—è¡¨
        pearson_threshold: çš®å°”é€Šç›¸å…³ç³»æ•°é˜ˆå€¼
        rfe_n_features: RFEé€‰æ‹©çš„ç‰¹å¾æ•°é‡
        rfe_estimator: RFEä½¿ç”¨çš„ä¼°è®¡å™¨ç±»å‹
    
    è¿”å›:
        final_selected_features: æœ€ç»ˆé€‰ä¸­çš„ç‰¹å¾åç§°åˆ—è¡¨
        selection_info: é€‰æ‹©ä¿¡æ¯å­—å…¸
    """
    print("\n" + "=" * 60)
    print("ç‰¹å¾é€‰æ‹©ï¼šæ¯”è¾ƒçš®å°”é€Šç›¸å…³æ€§å’ŒRFE")
    print("=" * 60)
    
    # æ–¹æ³•1ï¼šçš®å°”é€Šç›¸å…³æ€§
    pearson_features, pearson_indices, pearson_correlations = select_features_by_pearson(
        X, y, feature_names, threshold=pearson_threshold
    )
    
    # æ–¹æ³•2ï¼šRFE
    rfe_features, rfe_indices, rfe_ranking = select_features_by_rfe(
        X, y, feature_names, n_features=rfe_n_features, estimator_type=rfe_estimator
    )
    
    # æ¯”è¾ƒä¸¤ç§æ–¹æ³•
    print("\nğŸ“Š æ–¹æ³•æ¯”è¾ƒï¼š")
    print(f"  çš®å°”é€Šç›¸å…³æ€§é€‰æ‹©ï¼š{len(pearson_features)} ä¸ªç‰¹å¾")
    print(f"  RFEé€‰æ‹©ï¼š{len(rfe_features)} ä¸ªç‰¹å¾")
    
    # è®¡ç®—äº¤é›†å’Œå¹¶é›†
    pearson_set = set(pearson_features)
    rfe_set = set(rfe_features)
    intersection = pearson_set & rfe_set
    union = pearson_set | rfe_set
    
    print(f"  äº¤é›†ï¼š{len(intersection)} ä¸ªç‰¹å¾")
    print(f"  å¹¶é›†ï¼š{len(union)} ä¸ªç‰¹å¾")
    
    # é€‰æ‹©ç­–ç•¥ï¼šä¼˜å…ˆä½¿ç”¨äº¤é›†ï¼Œå¦‚æœäº¤é›†å¤ªå°‘åˆ™ä½¿ç”¨å¹¶é›†
    if len(intersection) >= 5:
        final_selected_features = list(intersection)
        selection_strategy = "äº¤é›†"
        print(f"\nâœ… ä½¿ç”¨äº¤é›†ç­–ç•¥ï¼š{len(final_selected_features)} ä¸ªç‰¹å¾")
    else:
        final_selected_features = list(union)
        selection_strategy = "å¹¶é›†"
        print(f"\nâœ… äº¤é›†ç‰¹å¾æ•°è¿‡å°‘ï¼Œä½¿ç”¨å¹¶é›†ç­–ç•¥ï¼š{len(final_selected_features)} ä¸ªç‰¹å¾")
    
    # è·å–æœ€ç»ˆç‰¹å¾çš„ç´¢å¼•
    final_indices = [feature_names.index(f) for f in final_selected_features]
    
    # æ„å»ºé€‰æ‹©ä¿¡æ¯
    selection_info = {
        "pearson_features": pearson_features,
        "pearson_indices": pearson_indices,
        "pearson_correlations": pearson_correlations,
        "rfe_features": rfe_features,
        "rfe_indices": rfe_indices,
        "rfe_ranking": rfe_ranking,
        "intersection": list(intersection),
        "union": list(union),
        "final_features": final_selected_features,
        "final_indices": final_indices,
        "selection_strategy": selection_strategy,
    }
    
    return final_selected_features, selection_info


def train_multiple_models(X_train, y_train, X_test, y_test, use_grid_search=True):
    """è®­ç»ƒå¤šä¸ªæ¨¡å‹"""
    results = {}
    
    # æ ‡å‡†åŒ–
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)
    
    # 1. SVR
    print("\nğŸ“Š è®­ç»ƒSVRæ¨¡å‹...")
    if use_grid_search:
        svr = SVR(kernel="rbf")
        grid_search = GridSearchCV(
            svr,
            svr_param_grid,
            cv=cv_folds,
            scoring="neg_mean_squared_error",
            n_jobs=-1,
            verbose=0,
        )
        grid_search.fit(X_train_scaled, y_train)
        svr_model = grid_search.best_estimator_
        print(f"  æœ€ä¼˜å‚æ•°ï¼š{grid_search.best_params_}")
    else:
        svr_model = SVR(kernel="rbf", C=10, gamma="scale", epsilon=0.1)
        svr_model.fit(X_train_scaled, y_train)
    
    y_pred_svr = svr_model.predict(X_test_scaled)
    results["SVR"] = {
        "model": svr_model,
        "scaler": scaler,
        "y_pred": y_pred_svr,
        "needs_scaling": True,
    }
    
    # 2. RandomForest
    print("\nğŸ“Š è®­ç»ƒRandomForestæ¨¡å‹...")
    rf_model = RandomForestRegressor(
        n_estimators=200, random_state=random_state, n_jobs=-1
    )
    rf_model.fit(X_train, y_train)
    y_pred_rf = rf_model.predict(X_test)
    results["RandomForest"] = {
        "model": rf_model,
        "scaler": None,
        "y_pred": y_pred_rf,
        "needs_scaling": False,
    }
    
    # 3. PLSR Baseline
    print("\nğŸ“Š è®­ç»ƒPLSR Baselineæ¨¡å‹...")
    # PLSRçš„ä¸»æˆåˆ†æ•°ä¸èƒ½è¶…è¿‡ç‰¹å¾æ•°æˆ–æ ·æœ¬æ•°çš„æœ€å°å€¼
    max_components = min(X_train_scaled.shape[0], X_train_scaled.shape[1])
    n_comp_range = range(1, min(21, max_components + 1))
    
    best_score = -np.inf
    optimal_n_comp = 1
    for n_comp in n_comp_range:
        plsr_temp = PLSRegression(n_components=n_comp)
        cv_scores = cross_val_score(
            plsr_temp, X_train_scaled, y_train, cv=cv_folds, scoring="neg_mean_squared_error", n_jobs=-1
        )
        mean_score = cv_scores.mean()
        if mean_score > best_score:
            best_score = mean_score
            optimal_n_comp = n_comp
    
    print(f"  æœ€ä¼˜ä¸»æˆåˆ†æ•°ï¼š{optimal_n_comp}")
    plsr_model = PLSRegression(n_components=optimal_n_comp)
    plsr_model.fit(X_train_scaled, y_train)
    y_pred_plsr = plsr_model.predict(X_test_scaled)
    results["PLSR"] = {
        "model": plsr_model,
        "scaler": scaler,
        "y_pred": y_pred_plsr,
        "needs_scaling": True,
    }
    
    # 4. GradientBoosting
    print("\nğŸ“Š è®­ç»ƒGradientBoostingæ¨¡å‹...")
    gb_model = GradientBoostingRegressor(
        n_estimators=100, random_state=random_state, max_depth=5
    )
    gb_model.fit(X_train, y_train)
    y_pred_gb = gb_model.predict(X_test)
    results["GradientBoosting"] = {
        "model": gb_model,
        "scaler": None,
        "y_pred": y_pred_gb,
        "needs_scaling": False,
    }
    
    # 5. XGBoost (å¦‚æœå¯ç”¨)
    if HAS_XGBOOST:
        print("\nğŸ“Š è®­ç»ƒXGBoostæ¨¡å‹...")
        xgb_model = XGBRegressor(
            n_estimators=100, random_state=random_state, n_jobs=-1
        )
        xgb_model.fit(X_train, y_train)
        y_pred_xgb = xgb_model.predict(X_test)
        results["XGBoost"] = {
            "model": xgb_model,
            "scaler": None,
            "y_pred": y_pred_xgb,
            "needs_scaling": False,
        }
    
    return results


def metrics_row(y_true, y_pred, name, n_features):
    mse = mean_squared_error(y_true, y_pred)
    rmse = np.sqrt(mse)
    mae = mean_absolute_error(y_true, y_pred)
    medae = median_absolute_error(y_true, y_pred)
    mape = np.mean(np.abs((y_true - y_pred) / (y_true + 1e-9))) * 100
    r2 = r2_score(y_true, y_pred)
    adj_r2 = 1 - (1 - r2) * (len(y_true) - 1) / (len(y_true) - n_features - 1)
    evs = explained_variance_score(y_true, y_pred)
    pearson_r, _ = pearsonr(y_true, y_pred)
    spearman_r, _ = spearmanr(y_true, y_pred)

    return {
        "æ¨¡å‹": name,
        "ç‰¹å¾æ•°": n_features,
        "MSE": mse,
        "RMSE": rmse,
        "MAE": mae,
        "MedAE": medae,
        "MAPE(%)": mape,
        "RÂ²": r2,
        "Adj_RÂ²": adj_r2,
        "EVS": evs,
        "Pearson_r": pearson_r,
        "Spearman_r": spearman_r,
    }


def plot_scatter(y_true, y_pred, title, save_path):
    plt.figure(figsize=(8, 6))
    plt.scatter(y_true, y_pred, alpha=0.6, edgecolor="k")
    plt.plot(
        [y_true.min(), y_true.max()],
        [y_true.min(), y_true.max()],
        "r--",
        lw=2,
        label="1:1 çº¿",
    )
    plt.xlabel("çœŸå®å…¨ç› (g/kg)")
    plt.ylabel("é¢„æµ‹å…¨ç› (g/kg)")
    plt.title(title)
    plt.legend()
    plt.grid(True, linestyle="--", alpha=0.5)
    plt.tight_layout()
    plt.savefig(save_path, dpi=300)
    plt.close()
    print(f"  âœ… æ•£ç‚¹å›¾å·²ä¿å­˜ï¼š{save_path}")


def plot_heatmap(df: pd.DataFrame, features, target):
    data = df[[target] + features]
    corr = data.corr(method="pearson")

    plt.figure(figsize=(12, 10))
    sns.heatmap(
        corr,
        annot=True,
        cmap="RdYlGn",
        center=0,
        fmt=".2f",
        cbar=True,
        square=True,
        linewidths=0.5,
        annot_kws={"size": 8},
    )
    plt.title(f"{target} ä¸å…‰è°±æŒ‡æ•°çš„ Pearson ç›¸å…³ç³»æ•°", fontsize=14)
    plt.tight_layout()
    out_path = output_dir / "pearson_corr_heatmap.png"
    plt.savefig(out_path, dpi=300)
    plt.close()
    print(f"âœ… Pearson çƒ­åŠ›å›¾å·²ä¿å­˜ï¼š{out_path}")


def apply_model_to_raster(
    model,
    scaler,
    selected_features: list,
    feature_columns: list,
    raster_path: Path,
    output_path: Path,
    needs_scaling: bool = True,
    remove_background: bool = True,
    background_threshold: float | None = None,
    check_nodata: bool = True,
) -> np.ndarray:
    """
    å°†è®­ç»ƒå¥½çš„æ¨¡å‹åº”ç”¨åˆ°æ …æ ¼å½±åƒï¼Œç”Ÿæˆç›åˆ†é¢„æµ‹æ …æ ¼
    
    å‚æ•°:
        model: è®­ç»ƒå¥½çš„æ¨¡å‹
        scaler: StandardScalerï¼ˆå¦‚æœä½¿ç”¨æ ‡å‡†åŒ–ï¼‰
        selected_features: é€‰ä¸­çš„ç‰¹å¾åç§°åˆ—è¡¨
        feature_columns: åŸå§‹ç‰¹å¾åˆ—ååˆ—è¡¨ï¼ˆç”¨äºåŒ¹é…æ …æ ¼æ³¢æ®µï¼‰
        raster_path: è¾“å…¥çš„æ …æ ¼å½±åƒè·¯å¾„
        output_path: è¾“å‡ºçš„é¢„æµ‹æ …æ ¼è·¯å¾„
        needs_scaling: æ¨¡å‹æ˜¯å¦éœ€è¦æ ‡å‡†åŒ–
    
    è¿”å›:
        é¢„æµ‹ç»“æœæ•°ç»„
    """
    print(f"\nğŸ“‚ è¯»å–æ …æ ¼å½±åƒï¼š{raster_path}")
    
    if not raster_path.exists():
        raise FileNotFoundError(f"æœªæ‰¾åˆ°æ …æ ¼æ–‡ä»¶ï¼š{raster_path}")
    
    with rasterio.open(raster_path) as src:
        height = src.height
        width = src.width
        count = src.count
        transform = src.transform
        crs = src.crs
        nodata = src.nodata
        
        print(f"   å°ºå¯¸: {height} Ã— {width}")
        print(f"   æ³¢æ®µæ•°: {count}")
        print(f"   CRS: {crs}")
        print(f"   Nodata: {nodata}")
        
        # è¯»å–æ‰€æœ‰æ³¢æ®µæ•°æ®
        all_bands = src.read()  # (bands, height, width)
        print("âœ… å½±åƒè¯»å–å®Œæˆ")
    
    # æ£€æŸ¥æ³¢æ®µæ•°æ˜¯å¦åŒ¹é…ç‰¹å¾æ•°
    if count < len(selected_features):
        raise ValueError(
            f"æ …æ ¼æ³¢æ®µæ•° ({count}) å°‘äºéœ€è¦çš„ç‰¹å¾æ•° ({len(selected_features)})"
        )
    
    print("\nğŸ“Š æå–ç‰¹å¾æ³¢æ®µ...")
    print(f"   éœ€è¦ç‰¹å¾: {len(selected_features)} ä¸ª")
    
    # å‡è®¾æ …æ ¼æ³¢æ®µçš„é¡ºåºå¯¹åº”ç‰¹å¾åˆ—çš„é¡ºåºï¼ˆä»æœ€åfeature_countåˆ—å¼€å§‹ï¼‰
    # ä¾‹å¦‚ï¼Œå¦‚æœfeature_count=30ï¼Œåˆ™ä½¿ç”¨æœ€å30ä¸ªæ³¢æ®µ
    if count >= feature_count:
        band_start_idx = count - feature_count
        band_indices = list(range(band_start_idx, count))
    else:
        band_indices = list(range(count))
    
    print(f"   ä½¿ç”¨æ³¢æ®µç´¢å¼•: {band_indices}")
    
    # æ„å»ºç‰¹å¾çŸ©é˜µï¼šé€‰æ‹©å¯¹åº”çš„æ³¢æ®µ
    feature_data = []
    feature_band_mapping = {}
    
    for feat_name in selected_features:
        # æ‰¾åˆ°è¯¥ç‰¹å¾åœ¨åŸå§‹ç‰¹å¾åˆ—ä¸­çš„ç´¢å¼•
        if feat_name in feature_columns:
            feat_idx = feature_columns.index(feat_name)
            # æ‰¾åˆ°å¯¹åº”çš„æ³¢æ®µç´¢å¼•ï¼ˆå‡è®¾ä»åå¾€å‰å¯¹åº”ï¼‰
            band_idx = band_start_idx + (len(feature_columns) - 1 - feat_idx)
            if band_idx < count:
                feature_data.append(all_bands[band_idx, :, :])
                feature_band_mapping[feat_name] = band_idx
                print(f"      {feat_name} â†’ æ³¢æ®µ {band_idx + 1}")
            else:
                raise ValueError(f"ç‰¹å¾ {feat_name} æ— æ³•åŒ¹é…åˆ°æœ‰æ•ˆçš„æ³¢æ®µ")
        else:
            print(f"âš ï¸ è­¦å‘Šï¼šç‰¹å¾ {feat_name} ä¸åœ¨ç‰¹å¾åˆ—ä¸­ï¼Œè·³è¿‡")
    
    if len(feature_data) == 0:
        raise ValueError("æ²¡æœ‰æ‰¾åˆ°ä»»ä½•åŒ¹é…çš„ç‰¹å¾æ³¢æ®µ")
    
    print(f"âœ… æˆåŠŸæå– {len(feature_data)} ä¸ªç‰¹å¾æ³¢æ®µ")
    
    # æ„å»ºç‰¹å¾çŸ©é˜µï¼š(height, width, n_features)
    feature_stack = np.stack(feature_data, axis=2)  # (height, width, n_features)
    feature_flat = feature_stack.reshape(-1, len(feature_data))  # (n_pixels, n_features)
    
    print(f"   ç‰¹å¾çŸ©é˜µå½¢çŠ¶: {feature_flat.shape}")
    
    # è¯†åˆ«èƒŒæ™¯åƒå…ƒ
    print("\nğŸ” è¯†åˆ«èƒŒæ™¯åƒå…ƒ...")
    
    # 1. æ£€æŸ¥æ— æ•ˆå€¼ï¼ˆNaN, Infï¼‰
    valid_mask = np.all(np.isfinite(feature_flat), axis=1)
    invalid_count = (~valid_mask).sum()
    if invalid_count > 0:
        print(f"   æ— æ•ˆå€¼ï¼ˆNaN/Infï¼‰: {invalid_count} ä¸ªåƒç´ ")
    
    # 2. æ£€æŸ¥åŸå§‹nodataå€¼ï¼ˆå¦‚æœå¯ç”¨ï¼‰
    background_mask = np.zeros(len(feature_flat), dtype=bool)
    if check_nodata and nodata is not None:
        # æ£€æŸ¥ç‰¹å¾æ³¢æ®µä¸­æ˜¯å¦æœ‰nodataå€¼
        nodata_mask = np.any(feature_flat == nodata, axis=1)
        background_mask = background_mask | nodata_mask
        nodata_count = nodata_mask.sum()
        if nodata_count > 0:
            print(f"   åŒ…å«nodataå€¼: {nodata_count} ä¸ªåƒç´ ")
    
    # 3. æ£€æŸ¥èƒŒæ™¯é˜ˆå€¼ï¼ˆå¦‚æœå¯ç”¨ï¼‰
    if background_threshold is not None:
        # å¦‚æœæ‰€æœ‰ç‰¹å¾æ³¢æ®µçš„å€¼éƒ½å°äºé˜ˆå€¼ï¼Œè§†ä¸ºèƒŒæ™¯
        low_value_mask = np.all(feature_flat < background_threshold, axis=1)
        background_mask = background_mask | low_value_mask
        low_value_count = low_value_mask.sum()
        if low_value_count > 0:
            print(f"   ä½äºé˜ˆå€¼ ({background_threshold}): {low_value_count} ä¸ªåƒç´ ")
    
    # 4. æ£€æŸ¥æ‰€æœ‰ç‰¹å¾å€¼éƒ½ä¸ºé›¶æˆ–æ¥è¿‘é›¶çš„åƒç´ ï¼ˆå¯èƒ½æ˜¯èƒŒæ™¯ï¼‰
    zero_mask = np.all(np.abs(feature_flat) < 1e-6, axis=1)
    background_mask = background_mask | zero_mask
    zero_count = zero_mask.sum()
    if zero_count > 0:
        print(f"   æ‰€æœ‰å€¼æ¥è¿‘é›¶: {zero_count} ä¸ªåƒç´ ")
    
    # åˆå¹¶æ‰€æœ‰èƒŒæ™¯åƒå…ƒåˆ¤æ–­
    if remove_background:
        # æœ‰æ•ˆåƒç´  = éæ— æ•ˆå€¼ ä¸” éèƒŒæ™¯åƒå…ƒ
        final_valid_mask = valid_mask & (~background_mask)
        background_total = (~final_valid_mask).sum()
        print(f"\n   èƒŒæ™¯åƒå…ƒæ€»æ•°: {background_total} ä¸ªåƒç´ ")
    else:
        final_valid_mask = valid_mask
        background_total = 0
        print("\n   èƒŒæ™¯åƒå…ƒå»é™¤: å·²ç¦ç”¨")
    
    feature_flat_valid = feature_flat[final_valid_mask]
    
    print(f"   æœ€ç»ˆæœ‰æ•ˆåƒç´ æ•°: {final_valid_mask.sum()}/{len(final_valid_mask)} "
          f"({100*final_valid_mask.sum()/len(final_valid_mask):.2f}%)")
    
    # æ ‡å‡†åŒ–ï¼ˆå¦‚æœéœ€è¦ï¼‰
    if needs_scaling and scaler is not None:
        print("   æ ‡å‡†åŒ–ç‰¹å¾...")
        feature_scaled = scaler.transform(feature_flat_valid)
    else:
        feature_scaled = feature_flat_valid
    
    # é¢„æµ‹
    print("   åº”ç”¨æ¨¡å‹é¢„æµ‹...")
    predicted_valid = model.predict(feature_scaled)
    
    # é‡å¡‘å›å½±åƒå½¢çŠ¶
    predicted_band = np.full((height, width), np.nan, dtype=np.float32)
    predicted_band_flat = predicted_band.ravel()
    predicted_band_flat[final_valid_mask] = predicted_valid
    predicted_band = predicted_band_flat.reshape(height, width)
    
    # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯ï¼ˆç›´æ¥ä½¿ç”¨ predicted_validï¼Œå®ƒå·²ç»æ˜¯æœ‰æ•ˆåƒç´ çš„é¢„æµ‹å€¼ï¼‰
    print(f"   é¢„æµ‹å€¼èŒƒå›´: [{predicted_valid.min():.4f}, {predicted_valid.max():.4f}]")
    print(f"   é¢„æµ‹å€¼å‡å€¼: {predicted_valid.mean():.4f}")
    print(f"   é¢„æµ‹å€¼æ ‡å‡†å·®: {predicted_valid.std():.4f}")
    
    # ä¿å­˜é¢„æµ‹æ …æ ¼
    print("\nğŸ’¾ ä¿å­˜é¢„æµ‹æ …æ ¼...")
    output_path.parent.mkdir(parents=True, exist_ok=True)
    
    nodata_value = -9999.0
    
    with rasterio.open(
        output_path,
        'w',
        driver='GTiff',
        height=height,
        width=width,
        count=1,
        dtype=np.float32,
        crs=crs,
        transform=transform,
        nodata=nodata_value,
        compress='lzw',
    ) as dst:
        # å°†NaNæ›¿æ¢ä¸ºnodataå€¼
        band_data_clean = np.where(np.isfinite(predicted_band), predicted_band, nodata_value)
        dst.write(band_data_clean, 1)
    
    print(f"âœ… é¢„æµ‹æ …æ ¼å·²ä¿å­˜ï¼š{output_path}")
    
    return predicted_band


def plot_salinity_spatial_distribution(
    raster_path: Path,
    output_path: Path,
    title: str = "åœŸå£¤ç›åˆ†ç©ºé—´åˆ†å¸ƒ",
    cmap_name: str = "YlOrRd",
    vmin: float | None = None,
    vmax: float | None = None,
) -> None:
    """
    ç»˜åˆ¶ç›åˆ†ç©ºé—´åˆ†å¸ƒå›¾ï¼ˆä½¿ç”¨åœ°ç†åæ ‡ï¼Œä¿æŒæ­£ç¡®çš„å®½é«˜æ¯”ï¼‰
    
    å‚æ•°:
        raster_path: ç›åˆ†é¢„æµ‹æ …æ ¼è·¯å¾„
        output_path: è¾“å‡ºå›¾åƒè·¯å¾„
        title: å›¾æ ‡é¢˜
        cmap_name: é¢œè‰²æ˜ å°„åç§°
        vmin: æœ€å°å€¼ï¼ˆç”¨äºé¢œè‰²æ˜ å°„ï¼‰
        vmax: æœ€å¤§å€¼ï¼ˆç”¨äºé¢œè‰²æ˜ å°„ï¼‰
    """
    print("\nğŸ¨ ç”Ÿæˆç©ºé—´åˆ†å¸ƒå›¾...")
    
    if not raster_path.exists():
        raise FileNotFoundError(f"æœªæ‰¾åˆ°æ …æ ¼æ–‡ä»¶ï¼š{raster_path}")
    
    # è¯»å–æ …æ ¼
    with rasterio.open(raster_path) as src:
        salinity_data = src.read(1)  # è¯»å–ç¬¬ä¸€ä¸ªæ³¢æ®µ
        crs = src.crs
        nodata = src.nodata
        bounds = src.bounds
        
        # å¤„ç†nodataå€¼
        if nodata is not None:
            salinity_data = np.where(salinity_data == nodata, np.nan, salinity_data)
        
        # è·å–æœ‰æ•ˆæ•°æ®èŒƒå›´
        valid_data = salinity_data[np.isfinite(salinity_data)]
        if len(valid_data) == 0:
            raise ValueError("æ …æ ¼ä¸­æ²¡æœ‰æœ‰æ•ˆæ•°æ®")
        
        if vmin is None:
            vmin = np.nanpercentile(salinity_data, 2)  # ä½¿ç”¨2%åˆ†ä½æ•°ä½œä¸ºæœ€å°å€¼
        if vmax is None:
            vmax = np.nanpercentile(salinity_data, 98)  # ä½¿ç”¨98%åˆ†ä½æ•°ä½œä¸ºæœ€å¤§å€¼
        
        # è®¡ç®—åœ°ç†èŒƒå›´ï¼ˆç”¨äºè®¾ç½®extentï¼‰
        extent = [bounds.left, bounds.right, bounds.bottom, bounds.top]
        
        # è®¡ç®—å®½é«˜æ¯”ï¼ˆåŸºäºåœ°ç†åæ ‡ï¼‰
        width_geo = bounds.right - bounds.left
        height_geo = bounds.top - bounds.bottom
        aspect_ratio = width_geo / height_geo if height_geo > 0 else 1.0
    
    # åˆ›å»ºå›¾å½¢ï¼ˆæ ¹æ®å®½é«˜æ¯”è°ƒæ•´å›¾å½¢å¤§å°ï¼‰
    fig_width = 12
    fig_height = fig_width / aspect_ratio
    fig, ax = plt.subplots(figsize=(fig_width, fig_height))
    
    # åˆ›å»ºæ©ç ï¼Œæ’é™¤NaNå€¼
    masked_data = np.ma.masked_invalid(salinity_data)
    
    # ç»˜åˆ¶æ …æ ¼ï¼ˆä½¿ç”¨åœ°ç†åæ ‡extentï¼‰
    im = ax.imshow(
        masked_data,
        cmap=cmap_name,
        vmin=vmin,
        vmax=vmax,
        interpolation='bilinear',
        extent=extent,  # ä½¿ç”¨åœ°ç†åæ ‡èŒƒå›´
        aspect='equal',  # ä¿æŒç­‰æ¯”ä¾‹ï¼Œè¿™æ ·åœ°ç†åæ ‡æ‰èƒ½æ­£ç¡®æ˜¾ç¤º
    )
    
    # æ·»åŠ é¢œè‰²æ¡
    cbar = plt.colorbar(im, ax=ax, fraction=0.046, pad=0.04)
    cbar.set_label('å…¨ç›é‡ (g/kg)', fontsize=12, rotation=270, labelpad=20)
    
    # è®¾ç½®æ ‡é¢˜å’Œæ ‡ç­¾ï¼ˆä½¿ç”¨åœ°ç†åæ ‡ï¼‰
    ax.set_title(title, fontsize=16, fontweight='bold', pad=20)
    
    # æ ¹æ®CRSè®¾ç½®åæ ‡è½´æ ‡ç­¾
    if crs is not None:
        if crs.is_geographic:
            ax.set_xlabel('ç»åº¦ (Â°)', fontsize=12)
            ax.set_ylabel('çº¬åº¦ (Â°)', fontsize=12)
        else:
            ax.set_xlabel(f'X åæ ‡ ({crs.linear_units})', fontsize=12)
            ax.set_ylabel(f'Y åæ ‡ ({crs.linear_units})', fontsize=12)
    else:
        ax.set_xlabel('X åæ ‡', fontsize=12)
        ax.set_ylabel('Y åæ ‡', fontsize=12)
    
    # æ·»åŠ ç»Ÿè®¡ä¿¡æ¯æ–‡æœ¬æ¡†
    stats_text = (
        f"æœ€å°å€¼: {valid_data.min():.2f} g/kg\n"
        f"æœ€å¤§å€¼: {valid_data.max():.2f} g/kg\n"
        f"å¹³å‡å€¼: {valid_data.mean():.2f} g/kg\n"
        f"æ ‡å‡†å·®: {valid_data.std():.2f} g/kg\n"
        f"æœ‰æ•ˆåƒç´ : {len(valid_data):,}"
    )
    ax.text(
        0.02, 0.98, stats_text,
        transform=ax.transAxes,
        fontsize=10,
        verticalalignment='top',
        bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.8),
    )
    
    # æ·»åŠ ç½‘æ ¼
    ax.grid(True, linestyle='--', alpha=0.3, color='gray')
    
    plt.tight_layout()
    
    # ä¿å­˜å›¾åƒ
    output_path.parent.mkdir(parents=True, exist_ok=True)
    plt.savefig(output_path, dpi=300, bbox_inches='tight')
    plt.close()
    
    print(f"âœ… ç©ºé—´åˆ†å¸ƒå›¾å·²ä¿å­˜ï¼š{output_path}")
    print(f"   å›¾åƒå°ºå¯¸: {fig_width:.1f} Ã— {fig_height:.1f} è‹±å¯¸ (å®½é«˜æ¯”: {aspect_ratio:.3f})")
    print(f"   åœ°ç†èŒƒå›´: X=[{bounds.left:.6f}, {bounds.right:.6f}], Y=[{bounds.bottom:.6f}, {bounds.top:.6f}]")


def build_descriptive_stats_table(
    df: pd.DataFrame,
    train_idx: pd.Index,
    test_idx: pd.Index,
    salt_col: str = "å…¨ç› (g/kg)",
    ec_col: str | None = "ç”µå¯¼ç‡ï¼ˆds/m)",
) -> pd.DataFrame:
    """æ„å»ºæè¿°æ€§ç»Ÿè®¡åˆ†æè¡¨"""
    
    # å…¨éƒ¨æ ·æœ¬
    salt_all = df[salt_col]
    has_ec = ec_col is not None and ec_col in df.columns
    ec_all = df[ec_col] if has_ec else None
    
    # å»ºæ¨¡æ ·æœ¬ï¼ˆè®­ç»ƒé›†ï¼‰
    salt_train = df.loc[train_idx, salt_col]
    ec_train = df.loc[train_idx, ec_col] if has_ec else None
    
    # éªŒè¯æ ·æœ¬ï¼ˆæµ‹è¯•é›†ï¼‰
    salt_test = df.loc[test_idx, salt_col]
    ec_test = df.loc[test_idx, ec_col] if has_ec else None
    
    # æ„å»ºç»Ÿè®¡è¡¨
    stats_list = []
    
    # å…¨ç›ç»Ÿè®¡
    stats_list.append({
        "ç»Ÿè®¡æŒ‡æ ‡": "æ•°é‡",
        "å…¨éƒ¨æ ·æœ¬": len(salt_all.dropna()),
        "å»ºæ¨¡æ ·æœ¬": len(salt_train.dropna()),
        "éªŒè¯æ ·æœ¬": len(salt_test.dropna()),
    })
    stats_list.append({
        "ç»Ÿè®¡æŒ‡æ ‡": f"å¹³å‡å€¼({salt_col})",
        "å…¨éƒ¨æ ·æœ¬": salt_all.mean(),
        "å»ºæ¨¡æ ·æœ¬": salt_train.mean(),
        "éªŒè¯æ ·æœ¬": salt_test.mean(),
    })
    stats_list.append({
        "ç»Ÿè®¡æŒ‡æ ‡": f"æœ€å¤§å€¼({salt_col})",
        "å…¨éƒ¨æ ·æœ¬": salt_all.max(),
        "å»ºæ¨¡æ ·æœ¬": salt_train.max(),
        "éªŒè¯æ ·æœ¬": salt_test.max(),
    })
    stats_list.append({
        "ç»Ÿè®¡æŒ‡æ ‡": f"æœ€å°å€¼({salt_col})",
        "å…¨éƒ¨æ ·æœ¬": salt_all.min(),
        "å»ºæ¨¡æ ·æœ¬": salt_train.min(),
        "éªŒè¯æ ·æœ¬": salt_test.min(),
    })
    stats_list.append({
        "ç»Ÿè®¡æŒ‡æ ‡": f"æ ‡å‡†å·®({salt_col})",
        "å…¨éƒ¨æ ·æœ¬": salt_all.std(),
        "å»ºæ¨¡æ ·æœ¬": salt_train.std(),
        "éªŒè¯æ ·æœ¬": salt_test.std(),
    })
    cv_all = (salt_all.std() / salt_all.mean()) * 100 if salt_all.mean() != 0 else np.nan
    cv_train = (salt_train.std() / salt_train.mean()) * 100 if salt_train.mean() != 0 else np.nan
    cv_test = (salt_test.std() / salt_test.mean()) * 100 if salt_test.mean() != 0 else np.nan
    stats_list.append({
        "ç»Ÿè®¡æŒ‡æ ‡": f"å˜å¼‚ç³»æ•°({salt_col})",
        "å…¨éƒ¨æ ·æœ¬": cv_all,
        "å»ºæ¨¡æ ·æœ¬": cv_train,
        "éªŒè¯æ ·æœ¬": cv_test,
    })
    
    # ç”µå¯¼ç‡ç»Ÿè®¡ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
    if has_ec:
        stats_list.append({
            "ç»Ÿè®¡æŒ‡æ ‡": "æ•°é‡",
            "å…¨éƒ¨æ ·æœ¬": len(ec_all.dropna()),
            "å»ºæ¨¡æ ·æœ¬": len(ec_train.dropna()),
            "éªŒè¯æ ·æœ¬": len(ec_test.dropna()),
        })
        stats_list.append({
            "ç»Ÿè®¡æŒ‡æ ‡": f"å¹³å‡å€¼({ec_col})",
            "å…¨éƒ¨æ ·æœ¬": ec_all.mean(),
            "å»ºæ¨¡æ ·æœ¬": ec_train.mean(),
            "éªŒè¯æ ·æœ¬": ec_test.mean(),
        })
        stats_list.append({
            "ç»Ÿè®¡æŒ‡æ ‡": f"æœ€å¤§å€¼({ec_col})",
            "å…¨éƒ¨æ ·æœ¬": ec_all.max(),
            "å»ºæ¨¡æ ·æœ¬": ec_train.max(),
            "éªŒè¯æ ·æœ¬": ec_test.max(),
        })
        stats_list.append({
            "ç»Ÿè®¡æŒ‡æ ‡": f"æœ€å°å€¼({ec_col})",
            "å…¨éƒ¨æ ·æœ¬": ec_all.min(),
            "å»ºæ¨¡æ ·æœ¬": ec_train.min(),
            "éªŒè¯æ ·æœ¬": ec_test.min(),
        })
        stats_list.append({
            "ç»Ÿè®¡æŒ‡æ ‡": f"æ ‡å‡†å·®({ec_col})",
            "å…¨éƒ¨æ ·æœ¬": ec_all.std(),
            "å»ºæ¨¡æ ·æœ¬": ec_train.std(),
            "éªŒè¯æ ·æœ¬": ec_test.std(),
        })
        cv_ec_all = (ec_all.std() / ec_all.mean()) * 100 if ec_all.mean() != 0 else np.nan
        cv_ec_train = (ec_train.std() / ec_train.mean()) * 100 if ec_train.mean() != 0 else np.nan
        cv_ec_test = (ec_test.std() / ec_test.mean()) * 100 if ec_test.mean() != 0 else np.nan
        stats_list.append({
            "ç»Ÿè®¡æŒ‡æ ‡": f"å˜å¼‚ç³»æ•°({ec_col})",
            "å…¨éƒ¨æ ·æœ¬": cv_ec_all,
            "å»ºæ¨¡æ ·æœ¬": cv_ec_train,
            "éªŒè¯æ ·æœ¬": cv_ec_test,
        })
    
    return pd.DataFrame(stats_list)


def main() -> None:
    ensure_paths()

    print("=" * 60)
    print("UAVå…‰è°±ç›åˆ†åæ¼”ï¼šçš®å°”é€Šç›¸å…³æ€§ + RFEç‰¹å¾é€‰æ‹© + å¤šæ¨¡å‹")
    print("=" * 60)
    print("ğŸ“Œ é…ç½®ä¿¡æ¯ï¼š")
    print(f"  - ç›®æ ‡åˆ—ï¼š{target_column}")
    print(f"  - çš®å°”é€Šç›¸å…³ç³»æ•°é˜ˆå€¼ï¼š{pearson_threshold}")
    print(f"  - RFEç‰¹å¾æ•°ï¼š{rfe_n_features}")
    print(f"  - RFEä¼°è®¡å™¨ï¼š{rfe_estimator}")

    df = pd.read_csv(input_csv)
    if target_column not in df.columns:
        raise KeyError(f"ç›®æ ‡åˆ— {target_column} ä¸å­˜åœ¨ã€‚")

    # æ ¹æ®åˆ—ä¸­åŒ…å«çš„å­—ç¬¦ä¸²è¿‡æ»¤æ ·æœ¬
    if filter_column is not None and exclude_strings:
        if filter_column not in df.columns:
            print(f"âš ï¸ è­¦å‘Šï¼šè¿‡æ»¤åˆ— '{filter_column}' ä¸å­˜åœ¨ï¼Œè·³è¿‡è¿‡æ»¤ã€‚")
        else:
            original_count = len(df)
            print("\nğŸ“Š æ ·æœ¬è¿‡æ»¤ï¼ˆæ ¹æ®åˆ—ä¸­åŒ…å«çš„å­—ç¬¦ä¸²ï¼‰...")
            print(f"   è¿‡æ»¤åˆ—ï¼š{filter_column}")
            print(f"   åŸå§‹æ ·æœ¬æ•°ï¼š{original_count}")
            print(f"   æ’é™¤å­—ç¬¦ä¸²ï¼š{exclude_strings}")
            
            # åˆ›å»ºè¿‡æ»¤æ©ç ï¼šæ’é™¤åˆ—å€¼ä¸­åŒ…å«æŒ‡å®šå­—ç¬¦ä¸²çš„æ ·æœ¬
            exclude_mask = pd.Series([False] * len(df), index=df.index)
            
            # ç»Ÿè®¡æ¯ä¸ªå­—ç¬¦ä¸²çš„æ•°é‡
            string_counts = {}
            
            for exclude_str in exclude_strings:
                # æ£€æŸ¥åˆ—å€¼ä¸­æ˜¯å¦åŒ…å«è¯¥å­—ç¬¦ä¸²
                str_mask = df[filter_column].astype(str).str.contains(exclude_str, na=False, regex=False)
                exclude_mask = exclude_mask | str_mask
                string_counts[exclude_str] = str_mask.sum()
            
            excluded_count = exclude_mask.sum()
            
            # è¿‡æ»¤æ•°æ®
            filtered_df = df[~exclude_mask].copy()
            df = filtered_df.reset_index(drop=True)
            
            print(f"   æ’é™¤æ ·æœ¬æ•°ï¼š{excluded_count}")
            print(f"   ä¿ç•™æ ·æœ¬æ•°ï¼š{len(df)}")
            print("   æ’é™¤çš„å­—ç¬¦ä¸²ç»Ÿè®¡ï¼š")
            for exclude_str in exclude_strings:
                count = string_counts.get(exclude_str, 0)
                print(f"      '{exclude_str}': {count} ä¸ªæ ·æœ¬")
            
            print("âœ… è¿‡æ»¤å®Œæˆ")

    X = select_features(df)
    y = df[target_column]

    # ç§»é™¤ç¼ºå¤±å€¼
    valid_mask = ~y.isna()
    X = X[valid_mask]
    y = y[valid_mask]
    df = df[valid_mask].reset_index(drop=True)
    X = X.reset_index(drop=True)
    y = y.reset_index(drop=True)

    print(f"\nâœ… æ•°æ®åŠ è½½å®Œæˆï¼š{len(X)} ä¸ªæ ·æœ¬ï¼Œ{X.shape[1]} ä¸ªç‰¹å¾")

    # åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=test_ratio, random_state=random_state
    )

    # è·å–è®­ç»ƒ/æµ‹è¯•é›†ç´¢å¼•ç”¨äºæè¿°æ€§ç»Ÿè®¡
    train_idx = X_train.index
    test_idx = X_test.index

    print("\nğŸ“Š æ•°æ®åˆ’åˆ†ï¼š")
    print(f"  - è®­ç»ƒé›†ï¼š{X_train.shape[0]} ä¸ªæ ·æœ¬")
    print(f"  - æµ‹è¯•é›†ï¼š{X_test.shape[0]} ä¸ªæ ·æœ¬")

    # ========== ç‰¹å¾é€‰æ‹©ï¼ˆçš®å°”é€Šç›¸å…³æ€§ + RFEï¼‰ ==========
    print("\n" + "=" * 60)
    print("æ­¥éª¤1ï¼šç‰¹å¾é€‰æ‹©ï¼ˆçš®å°”é€Šç›¸å…³æ€§ + RFEï¼‰")
    print("=" * 60)

    # è½¬æ¢ä¸ºnumpyæ•°ç»„ç”¨äºç‰¹å¾é€‰æ‹©
    X_train_array = X_train.values
    feature_names = X_train.columns.tolist()

    # æ¯”è¾ƒå¹¶é€‰æ‹©ç‰¹å¾
    selected_features, selection_info = compare_and_select_features(
        X_train_array,
        y_train.values,
        feature_names,
        pearson_threshold=pearson_threshold,
        rfe_n_features=rfe_n_features,
        rfe_estimator=rfe_estimator,
    )

    # æå–é€‰ä¸­çš„ç‰¹å¾
    X_train_selected = X_train[selected_features]
    X_test_selected = X_test[selected_features]

    print(f"âœ… ç‰¹å¾é€‰æ‹©å®Œæˆï¼š{X_train_selected.shape[1]} ä¸ªç‰¹å¾")
    print(f"   é€‰æ‹©ç­–ç•¥ï¼š{selection_info['selection_strategy']}")

    # ========== å¤šæ¨¡å‹è®­ç»ƒ ==========
    print("\n" + "=" * 60)
    print("æ­¥éª¤2ï¼šå¤šæ¨¡å‹è®­ç»ƒ")
    print("=" * 60)

    # åœ¨ç­›é€‰åçš„ç‰¹å¾ä¸Šè®­ç»ƒå¤šä¸ªæ¨¡å‹
    model_results = train_multiple_models(
        X_train_selected, y_train, X_test_selected, y_test, use_grid_search=True
    )

    # ========== æ¨¡å‹è¯„ä¼° ==========
    print("\n" + "=" * 60)
    print("æ­¥éª¤3ï¼šæ¨¡å‹è¯„ä¼°")
    print("=" * 60)

    perf_list = []
    for model_name, result in model_results.items():
        y_pred = result["y_pred"]
        metrics = metrics_row(y_test, y_pred, model_name, X_train_selected.shape[1])
        perf_list.append(metrics)
        
        print(f"\nğŸ“Š {model_name} æµ‹è¯•é›†ç»“æœï¼š")
        print(f"  RMSE: {metrics['RMSE']:.4f}")
        print(f"  MAE: {metrics['MAE']:.4f}")
        print(f"  RÂ²: {metrics['RÂ²']:.4f}")
        print(f"  Pearson_r: {metrics['Pearson_r']:.4f}")

    perf_df = pd.DataFrame(perf_list)

    # ========== å¯è§†åŒ– ==========
    print("\n" + "=" * 60)
    print("æ­¥éª¤4ï¼šå¯è§†åŒ–")
    print("=" * 60)

    # ç»˜åˆ¶æ•£ç‚¹å›¾
    for model_name, result in model_results.items():
        plot_scatter(
            y_test,
            result["y_pred"],
            f"{model_name} (ç‰¹å¾ç­›é€‰)",
            output_dir / f"scatter_{model_name.lower()}.png",
        )

    # ç»˜åˆ¶ç‰¹å¾é€‰æ‹©ç»“æœå¯¹æ¯”å›¾
    print("\nğŸ“Š ç»˜åˆ¶ç‰¹å¾é€‰æ‹©ç»“æœå¯¹æ¯”å›¾...")
    
    # åˆ›å»ºç‰¹å¾é€‰æ‹©ç»“æœDataFrame
    feature_selection_df = pd.DataFrame({
        "ç‰¹å¾": feature_names,
        "çš®å°”é€Šç›¸å…³ç³»æ•°": selection_info["pearson_correlations"],
        "RFEæ’å": selection_info["rfe_ranking"],
    })
    feature_selection_df["æ˜¯å¦é€‰ä¸­"] = feature_selection_df["ç‰¹å¾"].isin(selected_features)
    feature_selection_df = feature_selection_df.sort_values("çš®å°”é€Šç›¸å…³ç³»æ•°", key=abs, ascending=False)
    
    # ç»˜åˆ¶çš®å°”é€Šç›¸å…³ç³»æ•°æ¡å½¢å›¾
    plt.figure(figsize=(12, 8))
    top_features = feature_selection_df.head(20)
    colors = ['red' if sel else 'blue' for sel in top_features["æ˜¯å¦é€‰ä¸­"]]
    plt.barh(range(len(top_features)), top_features["çš®å°”é€Šç›¸å…³ç³»æ•°"].values, color=colors)
    plt.yticks(range(len(top_features)), top_features["ç‰¹å¾"].values)
    plt.xlabel("çš®å°”é€Šç›¸å…³ç³»æ•°")
    plt.title("Top 20 ç‰¹å¾çš®å°”é€Šç›¸å…³ç³»æ•°ï¼ˆçº¢è‰²=é€‰ä¸­ï¼Œè“è‰²=æœªé€‰ä¸­ï¼‰")
    plt.axvline(x=pearson_threshold, color='green', linestyle='--', label=f'é˜ˆå€¼={pearson_threshold}')
    plt.axvline(x=-pearson_threshold, color='green', linestyle='--')
    plt.legend()
    plt.gca().invert_yaxis()
    plt.tight_layout()
    pearson_plot_path = output_dir / "pearson_correlation_values.png"
    plt.savefig(pearson_plot_path, dpi=300)
    plt.close()
    print(f"âœ… çš®å°”é€Šç›¸å…³ç³»æ•°æ¡å½¢å›¾å·²ä¿å­˜ï¼š{pearson_plot_path}")
    
    # ç»˜åˆ¶RFEæ’åæ¡å½¢å›¾
    plt.figure(figsize=(12, 8))
    rfe_df = feature_selection_df.sort_values("RFEæ’å")
    top_rfe = rfe_df.head(20)
    colors_rfe = ['red' if sel else 'blue' for sel in top_rfe["æ˜¯å¦é€‰ä¸­"]]
    plt.barh(range(len(top_rfe)), top_rfe["RFEæ’å"].values, color=colors_rfe)
    plt.yticks(range(len(top_rfe)), top_rfe["ç‰¹å¾"].values)
    plt.xlabel("RFEæ’åï¼ˆ1=æœ€é‡è¦ï¼‰")
    plt.title("Top 20 ç‰¹å¾RFEæ’åï¼ˆçº¢è‰²=é€‰ä¸­ï¼Œè“è‰²=æœªé€‰ä¸­ï¼‰")
    plt.legend()
    plt.gca().invert_yaxis()
    plt.tight_layout()
    rfe_plot_path = output_dir / "rfe_ranking.png"
    plt.savefig(rfe_plot_path, dpi=300)
    plt.close()
    print(f"âœ… RFEæ’åæ¡å½¢å›¾å·²ä¿å­˜ï¼š{rfe_plot_path}")

    # ç»˜åˆ¶Pearsonçƒ­åŠ›å›¾
    plot_heatmap(df, X.columns.tolist(), target_column)

    # ========== ä¿å­˜é¢„æµ‹ç»“æœ ==========
    print("\n" + "=" * 60)
    print("æ­¥éª¤5ï¼šä¿å­˜ç»“æœ")
    print("=" * 60)

    # ç”Ÿæˆé¢„æµ‹ç»“æœDataFrame
    df_pred = df.copy()
    for model_name, result in model_results.items():
        model = result["model"]
        scaler = result["scaler"]
        needs_scaling = result["needs_scaling"]
        
        if needs_scaling and scaler is not None:
            X_all_scaled = scaler.transform(X[selected_features])
            df_pred[f"Pred_{model_name}"] = model.predict(X_all_scaled)
        else:
            df_pred[f"Pred_{model_name}"] = model.predict(X[selected_features])

    # ç”Ÿæˆæè¿°æ€§ç»Ÿè®¡åˆ†æè¡¨
    ec_col = "ç”µå¯¼ç‡ï¼ˆds/m)"
    if ec_col not in df.columns:
        possible_ec_cols = [
            col for col in df.columns
            if "ç”µå¯¼ç‡" in str(col) or "EC" in str(col).upper()
        ]
        if possible_ec_cols:
            ec_col = possible_ec_cols[0]
            print(f"âš ï¸ ä½¿ç”¨ '{ec_col}' ä½œä¸ºç”µå¯¼ç‡åˆ—")
        else:
            print("âš ï¸ æœªæ‰¾åˆ°ç”µå¯¼ç‡åˆ—ï¼Œä»…ç»Ÿè®¡å…¨ç›æ•°æ®")
            ec_col = None

    desc_stats_df = build_descriptive_stats_table(
        df, train_idx, test_idx, target_column, ec_col
    )

    # å…ƒä¿¡æ¯
    meta_df = pd.DataFrame(
        {
            "é”®": [
                "ç”Ÿæˆæ—¶é—´",
                "æ ·æœ¬æ•°",
                "åŸå§‹ç‰¹å¾æ•°",
                "ç­›é€‰åç‰¹å¾æ•°",
                "çš®å°”é€Šç›¸å…³æ€§ç‰¹å¾æ•°",
                "RFEç‰¹å¾æ•°",
                "äº¤é›†ç‰¹å¾æ•°",
                "é€‰æ‹©ç­–ç•¥",
                "çš®å°”é€Šé˜ˆå€¼",
                "RFEç‰¹å¾æ•°",
                "RFEä¼°è®¡å™¨",
            ],
            "å€¼": [
                datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                len(df),
                X.shape[1],
                X_train_selected.shape[1],
                len(selection_info["pearson_features"]),
                len(selection_info["rfe_features"]),
                len(selection_info["intersection"]),
                selection_info["selection_strategy"],
                pearson_threshold,
                rfe_n_features,
                rfe_estimator,
            ],
        }
    )

    # ä¿å­˜Excel
    excel_path = output_dir / excel_name
    with pd.ExcelWriter(excel_path, engine="openpyxl") as writer:
        feature_selection_df.to_excel(writer, sheet_name="ç‰¹å¾é€‰æ‹©ç»“æœ", index=False)
        
        # çš®å°”é€Šç›¸å…³æ€§ç»“æœ
        pearson_df = pd.DataFrame({
            "ç‰¹å¾": feature_names,
            "çš®å°”é€Šç›¸å…³ç³»æ•°": selection_info["pearson_correlations"],
            "æ˜¯å¦é€‰ä¸­": [f in selection_info["pearson_features"] for f in feature_names]
        }).sort_values("çš®å°”é€Šç›¸å…³ç³»æ•°", key=abs, ascending=False)
        pearson_df.to_excel(writer, sheet_name="çš®å°”é€Šç›¸å…³æ€§", index=False)
        
        # RFEç»“æœ
        rfe_df = pd.DataFrame({
            "ç‰¹å¾": feature_names,
            "RFEæ’å": selection_info["rfe_ranking"],
            "æ˜¯å¦é€‰ä¸­": [f in selection_info["rfe_features"] for f in feature_names]
        }).sort_values("RFEæ’å")
        rfe_df.to_excel(writer, sheet_name="RFEæ’å", index=False)
        
        # æ–¹æ³•æ¯”è¾ƒ
        comparison_df = pd.DataFrame({
            "ç‰¹å¾": feature_names,
            "çš®å°”é€Šé€‰ä¸­": [f in selection_info["pearson_features"] for f in feature_names],
            "RFEé€‰ä¸­": [f in selection_info["rfe_features"] for f in feature_names],
            "äº¤é›†": [f in selection_info["intersection"] for f in feature_names],
            "æœ€ç»ˆé€‰ä¸­": [f in selected_features for f in feature_names],
        })
        comparison_df.to_excel(writer, sheet_name="æ–¹æ³•æ¯”è¾ƒ", index=False)
        
        perf_df.to_excel(writer, sheet_name="æ¨¡å‹è¯„ä¼°", index=False)
        meta_df.to_excel(writer, sheet_name="å…ƒä¿¡æ¯", index=False)
        df_pred.to_excel(writer, sheet_name="é¢„æµ‹ç»“æœ", index=False)
        desc_stats_df.to_excel(writer, sheet_name="æè¿°æ€§ç»Ÿè®¡åˆ†æ", index=False)

    print(f"âœ… ç»“æœ Excel å·²è¾“å‡ºï¼š{excel_path}")

    # ========== ç”Ÿæˆç©ºé—´åˆ†å¸ƒå›¾ ==========
    if generate_spatial_map:
        print("\n" + "=" * 60)
        print("æ­¥éª¤6ï¼šç”Ÿæˆç©ºé—´åˆ†å¸ƒå›¾")
        print("=" * 60)
        
        if spatial_map_model not in model_results:
            print(f"âš ï¸ è­¦å‘Šï¼šæ¨¡å‹ '{spatial_map_model}' ä¸å­˜åœ¨ï¼Œå¯ç”¨æ¨¡å‹: {list(model_results.keys())}")
            print("   è·³è¿‡ç©ºé—´åˆ†å¸ƒå›¾ç”Ÿæˆ")
        else:
            result = model_results[spatial_map_model]
            model = result["model"]
            scaler = result["scaler"]
            needs_scaling = result["needs_scaling"]
            
            print(f"   ä½¿ç”¨æ¨¡å‹: {spatial_map_model}")
            print(f"   è¾“å…¥å½±åƒ: {raster_image_path}")
            
            try:
                # åº”ç”¨æ¨¡å‹åˆ°æ …æ ¼
                apply_model_to_raster(
                    model=model,
                    scaler=scaler,
                    selected_features=selected_features,
                    feature_columns=X.columns.tolist(),
                    raster_path=raster_image_path,
                    output_path=output_salinity_raster,
                    needs_scaling=needs_scaling,
                    remove_background=remove_background_pixels,
                    background_threshold=background_threshold,
                    check_nodata=check_nodata_values,
                )
                
                # ç”Ÿæˆç©ºé—´åˆ†å¸ƒå›¾
                spatial_map_path = output_dir / f"salinity_spatial_distribution_{spatial_map_model}.png"
                plot_salinity_spatial_distribution(
                    raster_path=output_salinity_raster,
                    output_path=spatial_map_path,
                    title=f"åœŸå£¤ç›åˆ†ç©ºé—´åˆ†å¸ƒå›¾ ({spatial_map_model} æ¨¡å‹é¢„æµ‹)",
                )
                
                print("\nâœ… ç©ºé—´åˆ†å¸ƒå›¾ç”Ÿæˆå®Œæˆï¼")
                print(f"   é¢„æµ‹æ …æ ¼: {output_salinity_raster}")
                print(f"   åˆ†å¸ƒå›¾: {spatial_map_path}")
                
            except Exception as e:
                print(f"âŒ ç”Ÿæˆç©ºé—´åˆ†å¸ƒå›¾æ—¶å‡ºé”™: {e}")
                import traceback
                traceback.print_exc()

    print("\nâœ… å¤„ç†å®Œæˆï¼")


if __name__ == "__main__":
    main()

